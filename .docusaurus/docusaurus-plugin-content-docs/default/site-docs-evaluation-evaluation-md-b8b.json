{
  "id": "Evaluation/Evaluation",
  "title": "Evaluation Page Documentation",
  "description": "The Evaluation Page in our platform provides a comprehensive suite of metrics to assess the performance and efficiency of specific workflows when interacting with Large Language Models (LLMs). This documentation explains the metrics and features available on the Evaluation Page, including how to use a Judge LLM to critically assess the output of your workflows.",
  "source": "@site/docs/Evaluation/Evaluation.md",
  "sourceDirName": "Evaluation",
  "slug": "/Evaluation/",
  "permalink": "/Pathlit-Docs/docs/Evaluation/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Evaluation/Evaluation.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 4,
  "frontMatter": {
    "sidebar_position": 4
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Evaluation",
    "permalink": "/Pathlit-Docs/docs/category/evaluation"
  },
  "next": {
    "title": "Tutorial - Basics",
    "permalink": "/Pathlit-Docs/docs/category/tutorial---basics"
  }
}