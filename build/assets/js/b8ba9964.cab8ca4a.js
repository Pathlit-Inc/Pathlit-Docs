"use strict";(self.webpackChunkpathlit_docs=self.webpackChunkpathlit_docs||[]).push([[278],{283:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var i=n(4848),o=n(8453);const s={sidebar_position:4},a="Evaluation Page Documentation",r={id:"Evaluation/Evaluation",title:"Evaluation Page Documentation",description:"The Evaluation Page in our platform provides a comprehensive suite of metrics to assess the performance and efficiency of specific workflows when interacting with Large Language Models (LLMs). This documentation explains the metrics and features available on the Evaluation Page, including how to use a Judge LLM to critically assess the output of your workflows.",source:"@site/docs/Evaluation/Evaluation.md",sourceDirName:"Evaluation",slug:"/Evaluation/",permalink:"/docs/Evaluation/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Evaluation/Evaluation.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Evaluation",permalink:"/docs/category/evaluation"},next:{title:"Tutorial - Basics",permalink:"/docs/category/tutorial---basics"}},l={},c=[{value:"How It Works",id:"how-it-works",level:2},{value:"Metrics Explained",id:"metrics-explained",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const t={h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"evaluation-page-documentation",children:"Evaluation Page Documentation"}),"\n",(0,i.jsx)(t.p,{children:"The Evaluation Page in our platform provides a comprehensive suite of metrics to assess the performance and efficiency of specific workflows when interacting with Large Language Models (LLMs). This documentation explains the metrics and features available on the Evaluation Page, including how to use a Judge LLM to critically assess the output of your workflows."}),"\n",(0,i.jsx)(t.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,i.jsx)(t.p,{children:"To evaluate a workflow:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Upload your workflow"})," to the tool. This should be a predefined sequence of tasks or operations you've set up using one or more LLMs."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Choose a Judge LLM."}),' Select an LLM that will serve as the "judge" for evaluating the output of your workflow. The Judge LLM assesses the effectiveness, accuracy, and relevance of the workflow\'s output against specific criteria or datasets.']}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"metrics-explained",children:"Metrics Explained"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Target LLM:"})," The specific Large Language Model your workflow is designed to utilize."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Dataset:"})," The data set against which your workflow's output is evaluated. This could be a collection of texts, questions, or any relevant data specific to your workflow's domain."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"BLEU (Bilingual Evaluation Understudy):"})," A metric for evaluating the quality of text that has been machine-translated from one language to another. Higher scores indicate better translation quality."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"MMLU (Massive Multitask Language Understanding):"})," Measures the language model's understanding across a broad set of tasks and domains."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"ROUGE (Recall-Oriented Understudy for Gisting Evaluation):"})," A set of metrics for evaluating automatic summarization of texts and the quality of machine-generated translations. It includes measures such as precision, recall, and F1 score."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"LLM:"})," The Large Language Model used in the workflow."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"LLM Judge:"})," The Large Language Model used to judge or evaluate the output of the workflow."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"TN (True Negative):"})," The number of instances correctly identified as not fulfilling the criteria."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"VDB (Vector Database):"})," Indicates whether the workflow utilizes a vector database for retrieving information or embedding."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"TK (Total Knowledge):"})," The total number of knowledge fragments or pieces of information the workflow managed to utilize effectively."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"TP (True Positive):"})," The number of instances correctly identified as fulfilling the criteria."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"MT (Model Tuning):"})," Indicates if the workflow involved any fine-tuning or customization of the model."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"FP (False Positive):"})," The number of instances incorrectly identified as fulfilling the criteria."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"PP (Presence Penalty):"})," A metric that penalizes the repetition of information in the workflow's output, encouraging diversity in responses."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"System Prompt:"})," The initial prompt or instruction given to the workflow to initiate the evaluation."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Details:"})," Additional information or comments about the evaluation process or outcome."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Cost:"})," The computational or financial cost associated with running the evaluation."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"DT (Decision Threshold):"})," The threshold at which the model's output is considered positive or negative, relevant or irrelevant."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Status:"})," The current status of the evaluation process (e.g., In Progress, Completed, Failed)."]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(t.p,{children:"The Evaluation Page offers a detailed and nuanced framework for assessing the performance of workflows using LLMs. By understanding and utilizing these metrics, users can refine their workflows, improve efficiency, and achieve more accurate and relevant outputs. The Judge LLM feature further enhances this process by providing an expert evaluation of the workflow's effectiveness."})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>r});var i=n(6540);const o={},s=i.createContext(o);function a(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);